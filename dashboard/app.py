"""
Streamlit Dashboard cho AIR GUARD
"""

import streamlit as st
import pandas as pd
import numpy as np
import json
import os
import matplotlib.pyplot as plt
import seaborn as sns
from pathlib import Path

# C·∫•u h√¨nh trang
st.set_page_config(
    page_title="AIR GUARD Dashboard",
    page_icon="üåç",
    layout="wide",
    initial_sidebar_state="expanded"
)

# Custom CSS
st.markdown("""
<style>
.main-header {
    font-size: 3rem;
    font-weight: bold;
    color: #028090;
    text-align: center;
    padding: 1rem 0;
}
.sub-header {
    font-size: 1.5rem;
    color: #00A896;
    font-weight: 600;
    margin-top: 1rem;
}
.metric-card {
    background-color: #F0F9FF;
    padding: 1.5rem;
    border-radius: 10px;
    border-left: 5px solid #028090;
}
</style>
""", unsafe_allow_html=True)


def load_metrics(filepath):
    """Load metrics from JSON file"""
    try:
        with open(filepath, 'r') as f:
            return json.load(f)
    except:
        return None


def load_history(filepath):
    """Load training history from JSON file"""
    try:
        with open(filepath, 'r') as f:
            return json.load(f)
    except:
        return None


def main():
    # Header
    st.markdown('<p class="main-header">üåç AIR GUARD</p>', unsafe_allow_html=True)
    st.markdown('<p style="text-align:center; color:#64748B; font-size:1.2rem;">D·ª± b√°o Ch·∫•t l∆∞·ª£ng Kh√¥ng kh√≠ v·ªõi H·ªçc B√°n gi√°m s√°t</p>', unsafe_allow_html=True)
    
    st.markdown("---")
    
    # Sidebar
    st.sidebar.title("üìä Navigation")
    page = st.sidebar.radio(
        "Ch·ªçn trang",
        ["T·ªïng quan", "Baseline Model", "Self-Training", "Co-Training", "So s√°nh"]
    )
    
    st.sidebar.markdown("---")
    st.sidebar.info("""
    **AIR GUARD** s·ª≠ d·ª•ng c√°c thu·∫≠t to√°n h·ªçc b√°n gi√°m s√°t ƒë·ªÉ d·ª± b√°o ch·∫•t l∆∞·ª£ng kh√¥ng kh√≠:
    - **Self-Training**: T·ª± hu·∫•n luy·ªán v·ªõi nh√£n gi·∫£
    - **Co-Training**: ƒê·ªìng hu·∫•n luy·ªán 2 models
    """)
    
    # Main content
    results_dir = Path('../results')
    
    if page == "T·ªïng quan":
        show_overview(results_dir)
    elif page == "Baseline Model":
        show_baseline(results_dir)
    elif page == "Self-Training":
        show_self_training(results_dir)
    elif page == "Co-Training":
        show_co_training(results_dir)
    elif page == "So s√°nh":
        show_comparison(results_dir)


def show_overview(results_dir):
    """Trang t·ªïng quan"""
    st.markdown('<p class="sub-header">üìã T·ªïng quan D·ª± √°n</p>', unsafe_allow_html=True)
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("""
        ### üéØ M·ª•c ti√™u
        D·ª± √°n AIR GUARD nh·∫±m d·ª± b√°o ch·∫•t l∆∞·ª£ng kh√¥ng kh√≠ (AQI) d·ª±a tr√™n n·ªìng ƒë·ªô PM2.5 
        v·ªõi d·ªØ li·ªáu c√≥ nh√£n khan hi·∫øm.
        
        ### üìä Ph∆∞∆°ng ph√°p
        - **Baseline**: HistGradientBoosting truy·ªÅn th·ªëng
        - **Self-Training**: T·ª± g√°n nh√£n v·ªõi ƒë·ªô tin c·∫≠y cao
        - **Co-Training**: 2 models v·ªõi 2 views ƒë·∫∑c tr∆∞ng
        
        ### üìà D·ªØ li·ªáu
        - Ngu·ªìn: Beijing PM2.5 Dataset
        - Ph√¢n chia: Train (< 2017), Test (‚â• 2017)
        - Labeled ratio: 10%
        """)
    
    with col2:
        st.markdown("""
        ### üèÜ Ti√™u ch√≠ AQI
        
        | M·ª©c | PM2.5 (Œºg/m¬≥) | M√¥ t·∫£ |
        |-----|---------------|-------|
        | Good | 0 - 12 | T·ªët |
        | Moderate | 12.1 - 35.4 | Trung b√¨nh |
        | Unhealthy (Sensitive) | 35.5 - 55.4 | Kh√¥ng l√†nh m·∫°nh cho nh√≥m nh·∫°y c·∫£m |
        | Unhealthy | 55.5 - 150.4 | Kh√¥ng l√†nh m·∫°nh |
        | Very Unhealthy | 150.5 - 250.4 | R·∫•t kh√¥ng l√†nh m·∫°nh |
        | Hazardous | > 250.4 | Nguy h·∫°i |
        """)
    
    st.markdown("---")
    
    # Load v√† hi·ªÉn th·ªã metrics n·∫øu c√≥
    metrics_baseline = load_metrics(results_dir / 'metrics_baseline.json')
    metrics_self = load_metrics(results_dir / 'metrics_self_training.json')
    metrics_co = load_metrics(results_dir / 'metrics_co_training.json')
    
    if metrics_baseline and metrics_self and metrics_co:
        st.markdown("### üéñÔ∏è K·∫øt qu·∫£ T·ªïng quan")
        
        col1, col2, col3 = st.columns(3)
        
        with col1:
            st.metric(
                "Baseline Accuracy",
                f"{metrics_baseline['accuracy']:.4f}",
                f"F1: {metrics_baseline['f1_macro']:.4f}"
            )
        
        with col2:
            delta_acc = metrics_self['accuracy'] - metrics_baseline['accuracy']
            st.metric(
                "Self-Training Accuracy",
                f"{metrics_self['accuracy']:.4f}",
                f"{delta_acc:+.4f} vs Baseline"
            )
        
        with col3:
            delta_acc = metrics_co['accuracy'] - metrics_baseline['accuracy']
            st.metric(
                "Co-Training Accuracy",
                f"{metrics_co['accuracy']:.4f}",
                f"{delta_acc:+.4f} vs Baseline"
            )
    else:
        st.warning("‚ö†Ô∏è Ch∆∞a c√≥ k·∫øt qu·∫£. Vui l√≤ng ch·∫°y `python src/main.py` tr∆∞·ªõc.")


def show_baseline(results_dir):
    """Hi·ªÉn th·ªã k·∫øt qu·∫£ Baseline"""
    st.markdown('<p class="sub-header">üìä Baseline Model</p>', unsafe_allow_html=True)
    
    metrics = load_metrics(results_dir / 'metrics_baseline.json')
    
    if metrics:
        st.markdown("### Overall Metrics")
        
        col1, col2, col3, col4 = st.columns(4)
        col1.metric("Accuracy", f"{metrics['accuracy']:.4f}")
        col2.metric("Precision (Macro)", f"{metrics['precision_macro']:.4f}")
        col3.metric("Recall (Macro)", f"{metrics['recall_macro']:.4f}")
        col4.metric("F1-Score (Macro)", f"{metrics['f1_macro']:.4f}")
        
        st.markdown("---")
        st.markdown("### Per-Class Performance")
        
        # T·∫°o DataFrame cho per-class metrics
        per_class_data = []
        for class_name, scores in metrics['per_class'].items():
            per_class_data.append({
                'Class': class_name,
                'Precision': scores['precision'],
                'Recall': scores['recall'],
                'F1-Score': scores['f1']
            })
        
        df_per_class = pd.DataFrame(per_class_data)
        st.dataframe(df_per_class, use_container_width=True)
        
        # Hi·ªÉn th·ªã confusion matrix n·∫øu c√≥
        cm_path = results_dir / 'cm_baseline.png'
        if cm_path.exists():
            st.markdown("### Confusion Matrix")
            st.image(str(cm_path), use_column_width=True)
    else:
        st.warning("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ Baseline.")


def show_self_training(results_dir):
    """Hi·ªÉn th·ªã k·∫øt qu·∫£ Self-Training"""
    st.markdown('<p class="sub-header">üîÑ Self-Training Model</p>', unsafe_allow_html=True)
    
    metrics = load_metrics(results_dir / 'metrics_self_training.json')
    history = load_history(results_dir / 'history_self_training.json')
    
    if metrics:
        st.markdown("### Overall Metrics")
        
        col1, col2, col3, col4 = st.columns(4)
        col1.metric("Accuracy", f"{metrics['accuracy']:.4f}")
        col2.metric("Precision (Macro)", f"{metrics['precision_macro']:.4f}")
        col3.metric("Recall (Macro)", f"{metrics['recall_macro']:.4f}")
        col4.metric("F1-Score (Macro)", f"{metrics['f1_macro']:.4f}")
        
        if history:
            st.markdown("---")
            st.markdown("### Training History")
            
            # Chuy·ªÉn history th√†nh DataFrame
            df_history = pd.DataFrame(history)
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.line_chart(df_history.set_index('iteration')['train_size'])
                st.caption("Training Set Size")
            
            with col2:
                if 'val_accuracy' in df_history.columns:
                    st.line_chart(df_history.set_index('iteration')[['val_accuracy', 'val_f1_macro']])
                    st.caption("Validation Performance")
        
        # Hi·ªÉn th·ªã plot n·∫øu c√≥
        plot_path = results_dir / 'self_training_history.png'
        if plot_path.exists():
            st.markdown("### Detailed History")
            st.image(str(plot_path), use_column_width=True)
        
        # Confusion matrix
        cm_path = results_dir / 'cm_self_training.png'
        if cm_path.exists():
            st.markdown("### Confusion Matrix")
            st.image(str(cm_path), use_column_width=True)
    else:
        st.warning("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ Self-Training.")


def show_co_training(results_dir):
    """Hi·ªÉn th·ªã k·∫øt qu·∫£ Co-Training"""
    st.markdown('<p class="sub-header">ü§ù Co-Training Model</p>', unsafe_allow_html=True)
    
    metrics = load_metrics(results_dir / 'metrics_co_training.json')
    history = load_history(results_dir / 'history_co_training.json')
    
    if metrics:
        st.markdown("### Overall Metrics")
        
        col1, col2, col3, col4 = st.columns(4)
        col1.metric("Accuracy", f"{metrics['accuracy']:.4f}")
        col2.metric("Precision (Macro)", f"{metrics['precision_macro']:.4f}")
        col3.metric("Recall (Macro)", f"{metrics['recall_macro']:.4f}")
        col4.metric("F1-Score (Macro)", f"{metrics['f1_macro']:.4f}")
        
        if history:
            st.markdown("---")
            st.markdown("### Training History")
            
            df_history = pd.DataFrame(history)
            
            col1, col2 = st.columns(2)
            
            with col1:
                st.line_chart(df_history.set_index('iteration')[['model_a_train_size', 'model_b_train_size']])
                st.caption("Training Set Sizes (Both Models)")
            
            with col2:
                if 'ensemble_val_accuracy' in df_history.columns:
                    st.line_chart(df_history.set_index('iteration')['ensemble_val_accuracy'])
                    st.caption("Ensemble Validation Accuracy")
        
        # Confusion matrix
        cm_path = results_dir / 'cm_co_training.png'
        if cm_path.exists():
            st.markdown("### Confusion Matrix")
            st.image(str(cm_path), use_column_width=True)
    else:
        st.warning("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y k·∫øt qu·∫£ Co-Training.")


def show_comparison(results_dir):
    """So s√°nh c√°c models"""
    st.markdown('<p class="sub-header">‚öñÔ∏è So s√°nh c√°c Models</p>', unsafe_allow_html=True)
    
    metrics_baseline = load_metrics(results_dir / 'metrics_baseline.json')
    metrics_self = load_metrics(results_dir / 'metrics_self_training.json')
    metrics_co = load_metrics(results_dir / 'metrics_co_training.json')
    
    if all([metrics_baseline, metrics_self, metrics_co]):
        # T·∫°o b·∫£ng so s√°nh
        comparison_data = {
            'Model': ['Baseline', 'Self-Training', 'Co-Training'],
            'Accuracy': [
                metrics_baseline['accuracy'],
                metrics_self['accuracy'],
                metrics_co['accuracy']
            ],
            'F1-Macro': [
                metrics_baseline['f1_macro'],
                metrics_self['f1_macro'],
                metrics_co['f1_macro']
            ],
            'Precision-Macro': [
                metrics_baseline['precision_macro'],
                metrics_self['precision_macro'],
                metrics_co['precision_macro']
            ],
            'Recall-Macro': [
                metrics_baseline['recall_macro'],
                metrics_self['recall_macro'],
                metrics_co['recall_macro']
            ]
        }
        
        df_comparison = pd.DataFrame(comparison_data)
        
        st.markdown("### üìä B·∫£ng So s√°nh")
        st.dataframe(df_comparison.style.highlight_max(axis=0, subset=['Accuracy', 'F1-Macro']), use_container_width=True)
        
        # Bi·ªÉu ƒë·ªì so s√°nh
        comp_path = results_dir / 'model_comparison.png'
        if comp_path.exists():
            st.markdown("### üìà Bi·ªÉu ƒë·ªì So s√°nh")
            st.image(str(comp_path), use_column_width=True)
        
        # Per-class comparison
        perclass_path = results_dir / 'per_class_comparison.png'
        if perclass_path.exists():
            st.markdown("### üìä So s√°nh T·ª´ng L·ªõp")
            st.image(str(perclass_path), use_column_width=True)
        
        # Insights
        st.markdown("---")
        st.markdown("### üí° Nh·∫≠n x√©t")
        
        best_model = df_comparison.loc[df_comparison['F1-Macro'].idxmax(), 'Model']
        best_f1 = df_comparison['F1-Macro'].max()
        baseline_f1 = df_comparison.loc[df_comparison['Model'] == 'Baseline', 'F1-Macro'].values[0]
        improvement = ((best_f1 - baseline_f1) / baseline_f1) * 100
        
        col1, col2 = st.columns(2)
        
        with col1:
            st.success(f"""
            **M√¥ h√¨nh t·ªët nh·∫•t**: {best_model}
            - F1-Score (Macro): {best_f1:.4f}
            - C·∫£i thi·ªán: +{improvement:.2f}% so v·ªõi Baseline
            """)
        
        with col2:
            if metrics_self['f1_macro'] > metrics_baseline['f1_macro']:
                st.info("‚úÖ Self-Training ƒë√£ c·∫£i thi·ªán hi·ªáu nƒÉng so v·ªõi Baseline")
            else:
                st.warning("‚ö†Ô∏è Self-Training ch∆∞a c·∫£i thi·ªán hi·ªáu nƒÉng")
            
            if metrics_co['f1_macro'] > metrics_self['f1_macro']:
                st.info("‚úÖ Co-Training t·ªët h∆°n Self-Training")
            else:
                st.info("‚ÑπÔ∏è Self-Training t·ªët h∆°n ho·∫∑c t∆∞∆°ng ƒë∆∞∆°ng Co-Training")
    else:
        st.warning("‚ö†Ô∏è Ch∆∞a ƒë·ªß k·∫øt qu·∫£ ƒë·ªÉ so s√°nh. Vui l√≤ng ch·∫°y t·∫•t c·∫£ c√°c models.")


if __name__ == "__main__":
    main()
